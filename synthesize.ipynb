{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights, efficientnet_v2_s, EfficientNet_V2_S_Weights\n",
    "from models.autoencoder import Decoder\n",
    "from models.sae import SimpleSAE, SimpleDAE, create_noise, delete_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from datasets.dataset import MvtecADDataset\n",
    "from losses.ssim_loss import SSIM_Loss\n",
    "from losses.gms_loss import MSGMS_Loss, MSGMS_Score\n",
    "from utils.early_stopping import EarlyStopping\n",
    "from utils.save import save_anomaly_map, plot_fig, save_model\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from torchvision.utils import save_image\n",
    "from eval.evaluate_experiment import *\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "\n",
    "class SimpleAD():\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        if args.seed is None:\n",
    "            args.seed = random.randint(1, 10000)\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(args.seed)\n",
    "        train_data = MvtecADDataset(root_dir=f\"mvtec_anomaly_detection_{args.img_size}\", split=\"train\", img_size=args.img_size)\n",
    "        img_nums = len(train_data)\n",
    "        valid_num = int(img_nums * 0.2)\n",
    "        train_num = img_nums - valid_num\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(train_data, [train_num, valid_num])\n",
    "\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "        self.valid_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "        # 모델 학습 설정\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = SimpleDAE().to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=args.lr)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "        if args.prefix is None:\n",
    "            self.save_name = f'{self.model.__class__.__name__}_{args.img_size}'\n",
    "        else:\n",
    "            self.save_name = f'{self.model.__class__.__name__}_{args.img_size}_{args.prefix}'\n",
    "        \n",
    "        os.makedirs(f'metrics/{self.save_name}', exist_ok=True)\n",
    "        with open(os.path.join(f'metrics/{self.save_name}', 'model_training_log.txt'), 'w') as f:\n",
    "            state = {k: v for k, v in args._get_kwargs()}\n",
    "            f.write(str(state))\n",
    "        \n",
    "        # fetch fixed data for debugging\n",
    "        x_normal_fixed, _, _, _, _ = next(iter(self.valid_loader))\n",
    "        self.x_normal_fixed = x_normal_fixed.to(self.device)\n",
    "\n",
    "        test_dataset = MvtecADDataset(root_dir=f\"mvtec_anomaly_detection_{args.img_size}\", split=\"test\", img_size=args.img_size)\n",
    "        random_indices = random.sample(range(len(test_dataset)), args.batch_size)\n",
    "        random_subset = Subset(test_dataset, random_indices)\n",
    "        random_loader = DataLoader(random_subset, batch_size=args.batch_size, shuffle=False)\n",
    "        x_test_fixed, _, _, _, _ = next(iter(random_loader))\n",
    "        self.x_test_fixed = x_test_fixed.to(self.device)   \n",
    "        \n",
    "    def train(self):\n",
    "        # 학습 루프\n",
    "        best_loss = 100000\n",
    "        early_stopping = EarlyStopping(patience=10)\n",
    "\n",
    "        for epoch in tqdm(range(self.args.epochs)):\n",
    "            \n",
    "            train_loss, train_l1_loss, train_l2_loss, train_gms_loss, train_ssim_loss, train_noise_loss = self._train()\n",
    "            valid_loss = self._eval()\n",
    "            \n",
    "            if epoch % 10 == 9:\n",
    "                save_sample = os.path.join(f'metrics/{self.save_name}', f'{epoch+1}-images.jpg')\n",
    "                save_sample2 = os.path.join(f'metrics/{self.save_name}', f'{epoch+1}test-images.jpg')\n",
    "                self.save_snapshot(self.x_normal_fixed, self.x_test_fixed, save_sample, save_sample2)\n",
    "                \n",
    "            self.scheduler.step(valid_loss / len(self.valid_loader))\n",
    "            early_stopping(val_loss=valid_loss / len(self.valid_loader))\n",
    "            best_loss = save_model(self.model, train_loss / len(self.train_loader), valid_loss / len(self.valid_loader), best_loss, epoch+1, self.save_name)\n",
    "            \n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{self.args.epochs}], Train Loss: {train_loss / len(self.train_loader):.4f}, Valid Loss {valid_loss / len(self.valid_loader):.4f}\")\n",
    "            print(f'''Train L1_Loss: {train_l1_loss / len(self.train_loader) * self.args.delta:.6f} L2_Loss: {train_l2_loss / len(self.train_loader)* self.args.gamma:.6f}\n",
    "                  GMS_Loss: {train_gms_loss / len(self.train_loader)* self.args.alpha:.6f} SSIM_Loss: {train_ssim_loss / len(self.train_loader)* self.args.beta:.6f}\n",
    "                  Noise_Loss: {train_noise_loss / len(self.train_loader)}''')\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "            \n",
    "    def _train(self):\n",
    "        self.model.train()\n",
    "        ssim = SSIM_Loss()\n",
    "        mse = nn.MSELoss()\n",
    "        msgms = MSGMS_Loss()\n",
    "        l1 = nn.L1Loss()\n",
    "        train_noise_loss = 0\n",
    "        train_loss = 0\n",
    "        train_l1_loss = 0\n",
    "        train_l2_loss = 0\n",
    "        train_gms_loss = 0\n",
    "        train_ssim_loss = 0\n",
    "        for images, _, _, _, _ in tqdm(self.train_loader):\n",
    "            if torch.isnan(images).any():\n",
    "                print(\"NaN detected in input images\")\n",
    "                continue  # NaN이 포함된 이미지는 건너뛰기\n",
    "            \n",
    "            if torch.isinf(images).any():\n",
    "                print(\"Inf detected in input images\")\n",
    "                continue  # Inf가 포함된 이미지는 건너뛰기\n",
    "            \n",
    "            images = images.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            \n",
    "            #==========================================================================\n",
    "            x = self.model.feature_extractor(images)\n",
    "            if np.random.rand() < 0.5:\n",
    "                noise = create_noise(x)\n",
    "                noise = delete_feature(noise, 0.9)\n",
    "            else:\n",
    "                noise = torch.zeros_like(x, device=self.device, requires_grad=False)\n",
    "            x = x + noise\n",
    "            _noise = self.model.noise_prediction(x)\n",
    "            x = x - _noise\n",
    "            outputs = self.model.decoder(x)    \n",
    "            #==========================================================================\n",
    "            # outputs = self.model.train_model(images)\n",
    "            \n",
    "            noise_loss = l1(noise, _noise)\n",
    "            l1_loss = l1(images, outputs)\n",
    "            l2_loss = mse(images, outputs)\n",
    "            gms_loss = msgms(images, outputs)\n",
    "            ssim_loss = ssim(images, outputs)\n",
    "            loss = noise_loss + l1_loss * self.args.delta + self.args.gamma * l2_loss + self.args.alpha * gms_loss + self.args.beta * ssim_loss\n",
    "            \n",
    "            train_noise_loss += noise_loss.item()\n",
    "            train_loss += loss.item()\n",
    "            train_l1_loss += l1_loss.item()\n",
    "            train_l2_loss += l2_loss.item()\n",
    "            train_gms_loss += gms_loss.item()\n",
    "            train_ssim_loss += ssim_loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        return train_loss, train_l1_loss, train_l2_loss, train_gms_loss, train_ssim_loss, train_noise_loss\n",
    "    \n",
    "    def _eval(self):\n",
    "        self.model.eval()\n",
    "        ssim = SSIM_Loss()\n",
    "        mse = nn.MSELoss()\n",
    "        msgms = MSGMS_Loss()\n",
    "        l1 = nn.L1Loss()\n",
    "        valid_l1_loss = 0\n",
    "        valid_l2_loss = 0\n",
    "        valid_gms_loss = 0\n",
    "        valid_ssim_loss = 0\n",
    "        valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, _, _, _, _ in tqdm(self.valid_loader):\n",
    "                images = images.to(self.device)\n",
    "                # outputs = self.model.train_model(images)\n",
    "\n",
    "                #==========================================================================\n",
    "                x = self.model.feature_extractor(images)\n",
    "                if np.random.rand() < 0.5:\n",
    "                    noise = create_noise(x)\n",
    "                    noise = delete_feature(noise, 0.9)\n",
    "                else:\n",
    "                    noise = torch.zeros_like(x, device=self.device, requires_grad=False)\n",
    "                \n",
    "                x = x + noise\n",
    "                _noise = self.model.noise_prediction(x)\n",
    "                x = x - _noise\n",
    "                outputs = self.model.decoder(x)   \n",
    "                #==========================================================================\n",
    "                \n",
    "                noise_loss = l1(noise, _noise)\n",
    "                l1_loss = l1(images, outputs)\n",
    "                l2_loss = mse(images, outputs)\n",
    "                gms_loss = msgms(images, outputs)\n",
    "                ssim_loss = ssim(images, outputs)\n",
    "                loss = noise_loss + self.args.delta * l1_loss + self.args.gamma * l2_loss + self.args.alpha * gms_loss + self.args.beta * ssim_loss\n",
    "\n",
    "                valid_l1_loss += l1_loss.item()\n",
    "                valid_l2_loss += l2_loss.item()\n",
    "                valid_gms_loss += gms_loss.item()\n",
    "                valid_ssim_loss += ssim_loss.item()\n",
    "                valid_loss += loss.item()\n",
    "        return valid_loss\n",
    "          \n",
    "    def _test(self, test_loader, root_anomaly_map_dir):\n",
    "        msgms_score = MSGMS_Score()\n",
    "        scores = []\n",
    "        test_imgs = []\n",
    "        gt_list = []\n",
    "        gt_mask_list = []\n",
    "        recon_imgs = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, masks, labels, _, image_paths in tqdm(test_loader):\n",
    "                score = 0\n",
    "                images = images.to(self.device)\n",
    "                test_imgs.extend(images.cpu().numpy())\n",
    "                gt_list.extend(labels.cpu().numpy())\n",
    "                gt_mask_list.extend(masks.cpu().numpy())\n",
    "                outputs = self.model(images)\n",
    "                # score = msgms_score(images, outputs).squeeze()\n",
    "                # score += torch.abs(images - outputs).mean(dim=1).squeeze()\n",
    "                score = F.mse_loss(images, outputs, reduction='none').mean(dim=1)\n",
    "                score = score.squeeze().cpu().numpy()\n",
    "                \n",
    "                for i in range(score.shape[0]):\n",
    "                    score[i] = gaussian_filter(score[i], sigma=7)\n",
    "\n",
    "                scores.extend(score)\n",
    "                recon_imgs.extend(outputs.cpu().numpy())\n",
    "                \n",
    "                # 배치의 각 이미지에 대해 anomaly map 저장 \n",
    "                for i in range(images.size(0)):\n",
    "                    image_path = image_paths[i]\n",
    "                    anomaly_map = score[i]\n",
    "                    save_anomaly_map(anomaly_map, image_path, root_anomaly_map_dir, img_size=self.args.img_size)\n",
    "                    \n",
    "        return scores, test_imgs, recon_imgs, gt_list, gt_mask_list\n",
    "    \n",
    "    def test(self, evaluated_objects, pro_integration_limit=0.3):\n",
    "        \n",
    "        assert 0.0 < pro_integration_limit <= 1.0\n",
    "        root_anomaly_map_dir=f'anomaly_maps/{self.save_name}'\n",
    "        output_dir=f'metrics/{self.save_name}'\n",
    "        evaluation_dict = dict()\n",
    "        # Keep track of the mean performance measures.\n",
    "        au_pros = []\n",
    "        au_rocs = []\n",
    "        \n",
    "        p_acs = []\n",
    "        p_prs = []\n",
    "        p_res = []\n",
    "        p_f1s = []\n",
    "        i_acs = []\n",
    "        i_prs = []\n",
    "        i_res = []\n",
    "        i_f1s = []\n",
    "        \n",
    "\n",
    "        # Evaluate each dataset object separately.\n",
    "        for obj in evaluated_objects:\n",
    "            print(f\"=== Evaluate {obj} ===\")\n",
    "            evaluation_dict[obj] = dict()\n",
    "            \n",
    "            test_dataset = MvtecADDataset(root_dir=f\"mvtec_anomaly_detection_{self.args.img_size}\", split=\"test\", img_size=self.args.img_size, object_names=[obj])\n",
    "            test_loader = DataLoader(test_dataset, batch_size=self.args.batch_size, shuffle=False)\n",
    "            scores, test_imgs, recon_imgs, gt_list, gt_mask_list = self._test(test_loader=test_loader, root_anomaly_map_dir=root_anomaly_map_dir)\n",
    "            scores = np.asarray(scores)\n",
    "\n",
    "            # Calculate the PRO and ROC curves.\n",
    "            au_pro, au_roc, pro_curve, roc_curve, pixel_level_metrics, image_level_metrics = \\\n",
    "                calculate_metrics(\n",
    "                    np.asanyarray(gt_mask_list).squeeze(axis=1),\n",
    "                    scores,\n",
    "                    pro_integration_limit)\n",
    "                \n",
    "            threshold = pixel_level_metrics['threshold']\n",
    "            save_dir = f'metrics/{self.save_name}/pictures_{obj}'\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            plot_fig(test_img=test_imgs, recon_imgs=recon_imgs, scores=scores, gts=gt_mask_list, threshold=threshold, save_dir=save_dir)\n",
    "            \n",
    "            evaluation_dict[obj]['au_pro'] = au_pro\n",
    "            evaluation_dict[obj]['classification_au_roc'] = au_roc\n",
    "            evaluation_dict[obj]['pixel_level_accuracy'] = pixel_level_metrics['accuracy']\n",
    "            evaluation_dict[obj]['pixel_level_precision'] = pixel_level_metrics['precision']\n",
    "            evaluation_dict[obj]['pixel_level_recall'] = pixel_level_metrics['recall']\n",
    "            evaluation_dict[obj]['pixel_level_f1_score'] = pixel_level_metrics['f1']\n",
    "            evaluation_dict[obj]['image_level_accuracy'] = image_level_metrics['accuracy']\n",
    "            evaluation_dict[obj]['image_level_precision'] = image_level_metrics['precision']\n",
    "            evaluation_dict[obj]['image_level_recall'] = image_level_metrics['recall']\n",
    "            evaluation_dict[obj]['image_level_f1_score'] = image_level_metrics['f1']\n",
    "            \n",
    "\n",
    "            evaluation_dict[obj]['classification_roc_curve_fpr'] = roc_curve[0]\n",
    "            evaluation_dict[obj]['classification_roc_curve_tpr'] = roc_curve[1]\n",
    "\n",
    "            # Keep track of the mean performance measures.\n",
    "            au_pros.append(au_pro)\n",
    "            au_rocs.append(au_roc)\n",
    "            p_acs.append(pixel_level_metrics['accuracy'])\n",
    "            p_prs.append(pixel_level_metrics['precision'])\n",
    "            p_res.append(pixel_level_metrics['recall'])\n",
    "            p_f1s.append(pixel_level_metrics['f1'])\n",
    "            i_acs.append(image_level_metrics['accuracy'])\n",
    "            i_prs.append(image_level_metrics['precision'])\n",
    "            i_res.append(image_level_metrics['recall'])\n",
    "            i_f1s.append(image_level_metrics['f1'])\n",
    "\n",
    "            print('\\n')\n",
    "\n",
    "        # Compute the mean of the performance measures.\n",
    "        evaluation_dict['mean_au_pro'] = np.mean(au_pros).item()\n",
    "        evaluation_dict['mean_classification_au_roc'] = np.mean(au_rocs).item()\n",
    "        \n",
    "        evaluation_dict['mean_pixel_level_accuracy'] = np.mean(p_acs).item()\n",
    "        evaluation_dict['mean_pixel_level_precision'] = np.mean(p_prs).item()\n",
    "        evaluation_dict['mean_pixel_level_recall'] = np.mean(p_res).item()\n",
    "        evaluation_dict['mean_pixel_level_f1_score'] = np.mean(p_f1s).item()\n",
    "        evaluation_dict['mean_image_level_accuracy'] = np.mean(i_acs).item()\n",
    "        evaluation_dict['mean_image_level_precision'] = np.mean(i_prs).item()\n",
    "        evaluation_dict['mean_image_level_recall'] = np.mean(i_res).item()\n",
    "        evaluation_dict['mean_image_level_f1_score'] = np.mean(i_f1s).item()\n",
    "\n",
    "        # If required, write evaluation metrics to drive.\n",
    "        if output_dir is not None:\n",
    "            makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            with open(path.join(output_dir, 'metrics.json'), 'w') as file:\n",
    "                json.dump(evaluation_dict, file, indent=4)\n",
    "\n",
    "            print(f\"Wrote metrics to {path.join(output_dir, 'metrics.json')}\")\n",
    "    \n",
    "    def load_model(self, state_dict_path):\n",
    "        self.model.load_state_dict(torch.load(state_dict_path, weights_only=True))\n",
    "    \n",
    "    def save_snapshot(self, x, x2, save_dir, save_dir2):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            x_fake_list = x\n",
    "            recon = self.model(x)\n",
    "            x_concat = torch.cat((x_fake_list, recon), dim=3)\n",
    "            save_image((x_concat.data.cpu()), save_dir, nrow=1, padding=0)\n",
    "            print(('Saved real and fake images into {}...'.format(save_dir)))\n",
    "\n",
    "            x_fake_list = x2\n",
    "            recon = self.model(x2)\n",
    "            x_concat = torch.cat((x_fake_list, recon), dim=3)\n",
    "            save_image((x_concat.data.cpu()), save_dir2, nrow=1, padding=0)\n",
    "            print(('Saved real and fake images into {}...'.format(save_dir2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leejinhyeok/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/home/leejinhyeok/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "sys.argv = ['script_name', '--prefix', 'cs', '--beta', '1', '--delta', '1']\n",
    "parser = argparse.ArgumentParser(description='Process some integers.')\n",
    "parser.add_argument('--prefix', type=str, default=None)\n",
    "parser.add_argument('--epochs', type=int, default=100)\n",
    "parser.add_argument('--lr', type=float, default=1e-3)\n",
    "parser.add_argument('--batch_size', type=int, default=16)\n",
    "parser.add_argument('--img_size', type=int, default=224)\n",
    "parser.add_argument('--alpha', type=float, default=1.0)\n",
    "parser.add_argument('--beta', type=float, default=1.0)\n",
    "parser.add_argument('--gamma', type=float, default=1.0)\n",
    "parser.add_argument('--delta', type=float, default=0.0)\n",
    "parser.add_argument('--seed', type=int, default=None, help='manual seed')\n",
    "args = parser.parse_args()\n",
    "\n",
    "exp = SimpleAD(args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluate bottle ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9a03265b79472298c3c1efe342ffc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 4164608 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.7143203213185346\n",
      "Threshold: 0.0033\n",
      "Pixel-level Accuracy: 0.9213\n",
      "Pixel-level Precision: 0.3526\n",
      "Pixel-level Recall: 0.4324\n",
      "Pixel-level F1 Score: 0.3884\n",
      "Image-level classification AU-ROC: 0.903968253968254\n",
      "Threshold: 0.0218\n",
      "Image-level Accuracy: 0.6867\n",
      "Image-level Precision: 1.0000\n",
      "Image-level Recall: 0.5873\n",
      "Image-level F1 Score: 0.7400\n",
      "\n",
      "\n",
      "=== Evaluate cable ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5343fb9137cb46a0bc9430ce3ea44ced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 7526400 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.5071816651032006\n",
      "Threshold: 0.0101\n",
      "Pixel-level Accuracy: 0.8814\n",
      "Pixel-level Precision: 0.1018\n",
      "Pixel-level Recall: 0.3995\n",
      "Pixel-level F1 Score: 0.1622\n",
      "Image-level classification AU-ROC: 0.47657421289355323\n",
      "Threshold: 0.0864\n",
      "Image-level Accuracy: 0.5133\n",
      "Image-level Precision: 0.5922\n",
      "Image-level Recall: 0.6630\n",
      "Image-level F1 Score: 0.6256\n",
      "\n",
      "\n",
      "=== Evaluate capsule ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003fb5c9ae304eec9da64630b56945e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 6623232 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.8134342597755158\n",
      "Threshold: 0.0029\n",
      "Pixel-level Accuracy: 0.9852\n",
      "Pixel-level Precision: 0.2830\n",
      "Pixel-level Recall: 0.4010\n",
      "Pixel-level F1 Score: 0.3318\n",
      "Image-level classification AU-ROC: 0.6972477064220185\n",
      "Threshold: 0.0043\n",
      "Image-level Accuracy: 0.4621\n",
      "Image-level Precision: 0.8958\n",
      "Image-level Recall: 0.3945\n",
      "Image-level F1 Score: 0.5478\n",
      "\n",
      "\n",
      "=== Evaluate carpet ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ade20a034228451e9eff298dfdffd2d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 5870592 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.36195309064647907\n",
      "Threshold: 0.0110\n",
      "Pixel-level Accuracy: 0.7337\n",
      "Pixel-level Precision: 0.0472\n",
      "Pixel-level Recall: 0.8142\n",
      "Pixel-level F1 Score: 0.0893\n",
      "Image-level classification AU-ROC: 0.45425361155698235\n",
      "Threshold: 0.0225\n",
      "Image-level Accuracy: 0.3846\n",
      "Image-level Precision: 0.7073\n",
      "Image-level Recall: 0.3258\n",
      "Image-level F1 Score: 0.4462\n",
      "\n",
      "\n",
      "=== Evaluate grid ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a7750c953254ce0bcfe11794b0843e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 3913728 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.9252122014224509\n",
      "Threshold: 0.0103\n",
      "Pixel-level Accuracy: 0.9909\n",
      "Pixel-level Precision: 0.3774\n",
      "Pixel-level Recall: 0.4888\n",
      "Pixel-level F1 Score: 0.4259\n",
      "Image-level classification AU-ROC: 0.9314954051796157\n",
      "Threshold: 0.0155\n",
      "Image-level Accuracy: 0.6282\n",
      "Image-level Precision: 1.0000\n",
      "Image-level Recall: 0.4912\n",
      "Image-level F1 Score: 0.6588\n",
      "\n",
      "\n",
      "=== Evaluate hazelnut ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d758561264e34bc885b35e88bf9c9fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 5519360 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.9095956766048356\n",
      "Threshold: 0.0048\n",
      "Pixel-level Accuracy: 0.9823\n",
      "Pixel-level Precision: 0.5698\n",
      "Pixel-level Recall: 0.6926\n",
      "Pixel-level F1 Score: 0.6252\n",
      "Image-level classification AU-ROC: 0.952857142857143\n",
      "Threshold: 0.0057\n",
      "Image-level Accuracy: 0.8455\n",
      "Image-level Precision: 0.9818\n",
      "Image-level Recall: 0.7714\n",
      "Image-level F1 Score: 0.8640\n",
      "\n",
      "\n",
      "=== Evaluate leather ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e474fc901ba34436899c7e92dc1c282a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 6221824 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.8408976855511145\n",
      "Threshold: 0.0047\n",
      "Pixel-level Accuracy: 0.9923\n",
      "Pixel-level Precision: 0.4179\n",
      "Pixel-level Recall: 0.4581\n",
      "Pixel-level F1 Score: 0.4370\n",
      "Image-level classification AU-ROC: 0.806046195652174\n",
      "Threshold: 0.0039\n",
      "Image-level Accuracy: 0.7258\n",
      "Image-level Precision: 0.9531\n",
      "Image-level Recall: 0.6630\n",
      "Image-level F1 Score: 0.7821\n",
      "\n",
      "\n",
      "=== Evaluate metal_nut ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e6cdca5d3c64b7f9d63cc773ea474a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 5770240 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.5862469937477777\n",
      "Threshold: 0.0038\n",
      "Pixel-level Accuracy: 0.8404\n",
      "Pixel-level Precision: 0.3912\n",
      "Pixel-level Recall: 0.6502\n",
      "Pixel-level F1 Score: 0.4885\n",
      "Image-level classification AU-ROC: 0.4081133919843597\n",
      "Threshold: 0.0197\n",
      "Image-level Accuracy: 0.2957\n",
      "Image-level Precision: 0.7308\n",
      "Image-level Recall: 0.2043\n",
      "Image-level F1 Score: 0.3193\n",
      "\n",
      "\n",
      "=== Evaluate pill ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5981e579be5b42d2926e0b8f46bf0085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 8379392 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.9125630803420108\n",
      "Threshold: 0.0021\n",
      "Pixel-level Accuracy: 0.9720\n",
      "Pixel-level Precision: 0.6163\n",
      "Pixel-level Recall: 0.4393\n",
      "Pixel-level F1 Score: 0.5130\n",
      "Image-level classification AU-ROC: 0.8235133660665576\n",
      "Threshold: 0.0049\n",
      "Image-level Accuracy: 0.5329\n",
      "Image-level Precision: 0.9846\n",
      "Image-level Recall: 0.4539\n",
      "Image-level F1 Score: 0.6214\n",
      "\n",
      "\n",
      "=== Evaluate screw ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915a9c5ed3714ae299236557b3116db8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 8028160 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.9067241199723258\n",
      "Threshold: 0.0079\n",
      "Pixel-level Accuracy: 0.9962\n",
      "Pixel-level Precision: 0.2118\n",
      "Pixel-level Recall: 0.1880\n",
      "Pixel-level F1 Score: 0.1992\n",
      "Image-level classification AU-ROC: 0.7993441278950605\n",
      "Threshold: 0.0085\n",
      "Image-level Accuracy: 0.5875\n",
      "Image-level Precision: 0.9492\n",
      "Image-level Recall: 0.4706\n",
      "Image-level F1 Score: 0.6292\n",
      "\n",
      "\n",
      "=== Evaluate tile ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af48471f6b3432186ac750bdd2e5a97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 5870592 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.400744009625497\n",
      "Threshold: 0.0117\n",
      "Pixel-level Accuracy: 0.8497\n",
      "Pixel-level Precision: 0.1678\n",
      "Pixel-level Recall: 0.2870\n",
      "Pixel-level F1 Score: 0.2118\n",
      "Image-level classification AU-ROC: 0.6428571428571429\n",
      "Threshold: 0.0283\n",
      "Image-level Accuracy: 0.4701\n",
      "Image-level Precision: 0.8235\n",
      "Image-level Recall: 0.3333\n",
      "Image-level F1 Score: 0.4746\n",
      "\n",
      "\n",
      "=== Evaluate toothbrush ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8459227dd2094df48bb6a3b772562fe7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 2107392 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.9072068996780784\n",
      "Threshold: 0.0082\n",
      "Pixel-level Accuracy: 0.9819\n",
      "Pixel-level Precision: 0.4311\n",
      "Pixel-level Recall: 0.5665\n",
      "Pixel-level F1 Score: 0.4896\n",
      "Image-level classification AU-ROC: 0.9638888888888889\n",
      "Threshold: 0.0163\n",
      "Image-level Accuracy: 0.5476\n",
      "Image-level Precision: 1.0000\n",
      "Image-level Recall: 0.3667\n",
      "Image-level F1 Score: 0.5366\n",
      "\n",
      "\n",
      "=== Evaluate transistor ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "000a0ab18d5c497290527b2509199c0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 5017600 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.6091955357731826\n",
      "Threshold: 0.0021\n",
      "Pixel-level Accuracy: 0.8645\n",
      "Pixel-level Precision: 0.1755\n",
      "Pixel-level Recall: 0.4940\n",
      "Pixel-level F1 Score: 0.2590\n",
      "Image-level classification AU-ROC: 0.6408333333333333\n",
      "Threshold: 0.0168\n",
      "Image-level Accuracy: 0.4600\n",
      "Image-level Precision: 0.4103\n",
      "Image-level Recall: 0.8000\n",
      "Image-level F1 Score: 0.5424\n",
      "\n",
      "\n",
      "=== Evaluate wood ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78c8e935764248aa9c2671b79607f83f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 3963904 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.7185436877606248\n",
      "Threshold: 0.0035\n",
      "Pixel-level Accuracy: 0.9595\n",
      "Pixel-level Precision: 0.4734\n",
      "Pixel-level Recall: 0.4369\n",
      "Pixel-level F1 Score: 0.4544\n",
      "Image-level classification AU-ROC: 0.9333333333333333\n",
      "Threshold: 0.0093\n",
      "Image-level Accuracy: 0.5063\n",
      "Image-level Precision: 1.0000\n",
      "Image-level Recall: 0.3500\n",
      "Image-level F1 Score: 0.5185\n",
      "\n",
      "\n",
      "=== Evaluate zipper ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "004937ec1cca41bb8ae078f1823e0eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 7576576 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.8222804026085917\n",
      "Threshold: 0.0044\n",
      "Pixel-level Accuracy: 0.9760\n",
      "Pixel-level Precision: 0.4386\n",
      "Pixel-level Recall: 0.5774\n",
      "Pixel-level F1 Score: 0.4985\n",
      "Image-level classification AU-ROC: 0.8747373949579833\n",
      "Threshold: 0.0188\n",
      "Image-level Accuracy: 0.4636\n",
      "Image-level Precision: 0.9524\n",
      "Image-level Recall: 0.3361\n",
      "Image-level F1 Score: 0.4969\n",
      "\n",
      "\n",
      "Wrote metrics to metrics/SimpleDAE_224_cs/metrics.json\n"
     ]
    }
   ],
   "source": [
    "OBJECT_NAMES = ['bottle', 'cable', 'capsule', 'carpet', 'grid',\n",
    "                'hazelnut', 'leather', 'metal_nut', 'pill', 'screw',\n",
    "                'tile', 'toothbrush', 'transistor', 'wood', 'zipper']\n",
    "exp.load_model('save/SimpleDAE_224_cs')\n",
    "exp.test(evaluated_objects=OBJECT_NAMES, pro_integration_limit=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
