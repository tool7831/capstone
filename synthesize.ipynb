{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights, efficientnet_v2_s, EfficientNet_V2_S_Weights\n",
    "from models.autoencoder import Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_noise(true_feats, mix_noise=1, noise_std=0.1, device='cuda'):\n",
    "    B, C, H, W = true_feats.shape\n",
    "    # 1. 노이즈 인덱스 생성 (B 크기)\n",
    "    noise_idxs = torch.randint(0, mix_noise, size=(B,))\n",
    "    # 2. 원-핫 인코딩 (B, K)\n",
    "    noise_one_hot = F.one_hot(noise_idxs, num_classes=mix_noise)\n",
    "\n",
    "    # 3. 가우시안 노이즈 생성 (B, K, C, H, W)\n",
    "    noise = torch.stack([\n",
    "        torch.normal(0, noise_std * 1.1**k, size=(B, C, H, W))\n",
    "        for k in range(mix_noise)\n",
    "    ], dim=1)\n",
    "\n",
    "    noise = (noise * noise_one_hot.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)).sum(dim=1)\n",
    "\n",
    "    return noise.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleSAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.feature_extractor = efficientnet_v2_s(EfficientNet_V2_S_Weights.DEFAULT).features\n",
    "        self.feature_adaptor = nn.Linear(1280, 1280)\n",
    "        self.decoder = Decoder([1280, 640, 256, 128, 64, 4])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x = self.feature_extractor(x)\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = self.feature_adaptor(x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = self.decoder(x)\n",
    "        mask = x[:,3]\n",
    "        x = x[:,0:3]\n",
    "        return x, mask\n",
    "    \n",
    "    def train_model(self, x):\n",
    "        with torch.no_grad():\n",
    "            x = self.feature_extractor(x)\n",
    "        x = x.permute(0, 2, 3, 1)   # B, H, W, C\n",
    "        x = self.feature_adaptor(x) \n",
    "        x = x.permute(0, 3, 1, 2)   # B, C, H, W\n",
    "        noise = create_noise(x, device=self.device)\n",
    "        x = x + noise\n",
    "        x = self.decoder(x)\n",
    "        mask = x[:,3]\n",
    "        x = x[:,0:3]\n",
    "        return x, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from datasets.dataset import MvtecADDataset\n",
    "from losses.ssim_loss import SSIM_Loss\n",
    "from losses.gms_loss import MSGMS_Loss, MSGMS_Score\n",
    "from utils.early_stopping import EarlyStopping\n",
    "from utils.save import save_anomaly_map, plot_fig, save_model\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from torchvision.utils import save_image\n",
    "from eval.evaluate_experiment import *\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import json\n",
    "\n",
    "class SimpleAD():\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        if args.seed is None:\n",
    "            args.seed = random.randint(1, 10000)\n",
    "        random.seed(args.seed)\n",
    "        torch.manual_seed(args.seed)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed_all(args.seed)\n",
    "        train_data = MvtecADDataset(root_dir=f\"mvtec_anomaly_detection_{args.img_size}\", split=\"train\", img_size=args.img_size)\n",
    "        img_nums = len(train_data)\n",
    "        valid_num = int(img_nums * 0.2)\n",
    "        train_num = img_nums - valid_num\n",
    "        train_dataset, val_dataset = torch.utils.data.random_split(train_data, [train_num, valid_num])\n",
    "\n",
    "        self.train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "        self.valid_loader = DataLoader(val_dataset, batch_size=args.batch_size, shuffle=True)\n",
    "\n",
    "        # 모델 학습 설정\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = SimpleSAE().to(self.device)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=args.lr)\n",
    "        self.scheduler = ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "        if args.prefix is None:\n",
    "            self.save_name = f'{self.model.__class__.__name__}_{args.img_size}'\n",
    "        else:\n",
    "            self.save_name = f'{self.model.__class__.__name__}_{args.img_size}_{args.prefix}'\n",
    "        \n",
    "        os.makedirs(f'metrics/{self.save_name}', exist_ok=True)\n",
    "        with open(os.path.join(f'metrics/{self.save_name}', 'model_training_log.txt'), 'w') as f:\n",
    "            state = {k: v for k, v in args._get_kwargs()}\n",
    "            f.write(str(state))\n",
    "        \n",
    "        # fetch fixed data for debugging\n",
    "        x_normal_fixed, _, _, _, _ = next(iter(self.valid_loader))\n",
    "        self.x_normal_fixed = x_normal_fixed.to(self.device)\n",
    "\n",
    "        test_dataset = MvtecADDataset(root_dir=f\"mvtec_anomaly_detection_{args.img_size}\", split=\"test\", img_size=args.img_size)\n",
    "        random_indices = random.sample(range(len(test_dataset)), args.batch_size)\n",
    "        random_subset = Subset(test_dataset, random_indices)\n",
    "        random_loader = DataLoader(random_subset, batch_size=args.batch_size, shuffle=False)\n",
    "        x_test_fixed, _, _, _, _ = next(iter(random_loader))\n",
    "        self.x_test_fixed = x_test_fixed.to(self.device)   \n",
    "        \n",
    "    def train(self):\n",
    "        # 학습 루프\n",
    "        best_loss = 100000\n",
    "        early_stopping = EarlyStopping(patience=10)\n",
    "\n",
    "        for epoch in tqdm(range(self.args.epochs)):\n",
    "            \n",
    "            train_loss, train_l1_loss, train_l2_loss, train_gms_loss, train_ssim_loss = self._train()\n",
    "            valid_loss = self._eval()\n",
    "            \n",
    "            if epoch % 10 == 9:\n",
    "                save_sample = os.path.join(f'metrics/{self.save_name}', f'{epoch+1}-images.jpg')\n",
    "                save_sample2 = os.path.join(f'metrics/{self.save_name}', f'{epoch+1}test-images.jpg')\n",
    "                self.save_snapshot(self.x_normal_fixed, self.x_test_fixed, save_sample, save_sample2)\n",
    "                \n",
    "            self.scheduler.step(valid_loss / len(self.valid_loader))\n",
    "            early_stopping(val_loss=valid_loss / len(self.valid_loader))\n",
    "            best_loss = save_model(self.model, train_loss / len(self.train_loader), valid_loss / len(self.valid_loader), best_loss, epoch+1, self.save_name)\n",
    "            \n",
    "            \n",
    "            print(f\"Epoch [{epoch+1}/{self.args.epochs}], Train Loss: {train_loss / len(self.train_loader):.4f}, Valid Loss {valid_loss / len(self.valid_loader):.4f}\")\n",
    "            print(f'Train L1_Loss: {train_l1_loss / len(self.train_loader) * self.args.delta:.6f} L2_Loss: {train_l2_loss / len(self.train_loader)* self.args.gamma:.6f} GMS_Loss: {train_gms_loss / len(self.train_loader)* self.args.alpha:.6f} SSIM_Loss: {train_ssim_loss / len(self.train_loader)* self.args.beta:.6f}')\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "            \n",
    "    def _train(self):\n",
    "        self.model.train()\n",
    "        ssim = SSIM_Loss()\n",
    "        mse = nn.MSELoss()\n",
    "        msgms = MSGMS_Loss()\n",
    "        l1 = nn.L1Loss()\n",
    "        train_loss = 0\n",
    "        train_l1_loss = 0\n",
    "        train_l2_loss = 0\n",
    "        train_gms_loss = 0\n",
    "        train_ssim_loss = 0\n",
    "        for images, _, _, _, _ in tqdm(self.train_loader):\n",
    "            if torch.isnan(images).any():\n",
    "                print(\"NaN detected in input images\")\n",
    "                continue  # NaN이 포함된 이미지는 건너뛰기\n",
    "            \n",
    "            if torch.isinf(images).any():\n",
    "                print(\"Inf detected in input images\")\n",
    "                continue  # Inf가 포함된 이미지는 건너뛰기\n",
    "            \n",
    "            images = images.to(self.device)\n",
    "            self.optimizer.zero_grad()\n",
    "            outputs, mask = self.model.train_model(images)\n",
    "            l1_loss = l1(images, outputs)\n",
    "            l2_loss = mse(images, outputs)\n",
    "            gms_loss = msgms(images, outputs)\n",
    "            ssim_loss = ssim(images, outputs)\n",
    "            loss = l1_loss * self.args.delta + self.args.gamma * l2_loss + self.args.alpha * gms_loss + self.args.beta * ssim_loss\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_l1_loss += l1_loss.item()\n",
    "            train_l2_loss += l2_loss.item()\n",
    "            train_gms_loss += gms_loss.item()\n",
    "            train_ssim_loss += ssim_loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        return train_loss, train_l1_loss, train_l2_loss, train_gms_loss, train_ssim_loss\n",
    "    \n",
    "    def _eval(self):\n",
    "        self.model.eval()\n",
    "        ssim = SSIM_Loss()\n",
    "        mse = nn.MSELoss()\n",
    "        msgms = MSGMS_Loss()\n",
    "        l1 = nn.L1Loss()\n",
    "        valid_l1_loss = 0\n",
    "        valid_l2_loss = 0\n",
    "        valid_gms_loss = 0\n",
    "        valid_ssim_loss = 0\n",
    "        valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, _, _, _, _ in tqdm(self.valid_loader):\n",
    "                images = images.to(self.device)\n",
    "                outputs, mask = self.model.train_model(images)\n",
    "\n",
    "                l1_loss = l1(images, outputs)\n",
    "                l2_loss = mse(images, outputs)\n",
    "                gms_loss = msgms(images, outputs)\n",
    "                ssim_loss = ssim(images, outputs)\n",
    "                loss = self.args.delta * l1_loss + self.args.gamma * l2_loss + self.args.alpha * gms_loss + self.args.beta * ssim_loss\n",
    "\n",
    "                valid_l1_loss += l1_loss.item()\n",
    "                valid_l2_loss += l2_loss.item()\n",
    "                valid_gms_loss += gms_loss.item()\n",
    "                valid_ssim_loss += ssim_loss.item()\n",
    "                valid_loss += loss.item()\n",
    "        return valid_loss\n",
    "          \n",
    "    def _test(self, test_loader, root_anomaly_map_dir):\n",
    "        msgms_score = MSGMS_Score()\n",
    "        scores = []\n",
    "        test_imgs = []\n",
    "        gt_list = []\n",
    "        gt_mask_list = []\n",
    "        recon_imgs = []\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for images, masks, labels, _, image_paths in tqdm(test_loader):\n",
    "                score = 0\n",
    "                images = images.to(self.device)\n",
    "                test_imgs.extend(images.cpu().numpy())\n",
    "                gt_list.extend(labels.cpu().numpy())\n",
    "                gt_mask_list.extend(masks.cpu().numpy())\n",
    "                outputs, mask = self.model(images)\n",
    "                score = msgms_score(images, outputs)\n",
    "                # score = F.mse_loss(images, outputs, reduction='none').mean(dim=1)\n",
    "                score = score.squeeze().cpu().numpy()\n",
    "                \n",
    "                for i in range(score.shape[0]):\n",
    "                    score[i] = gaussian_filter(score[i], sigma=7)\n",
    "\n",
    "                scores.extend(score)\n",
    "                recon_imgs.extend(outputs.cpu().numpy())\n",
    "                \n",
    "                # 배치의 각 이미지에 대해 anomaly map 저장 \n",
    "                for i in range(images.size(0)):\n",
    "                    image_path = image_paths[i]\n",
    "                    anomaly_map = score[i]\n",
    "                    save_anomaly_map(anomaly_map, image_path, root_anomaly_map_dir, img_size=self.args.img_size)\n",
    "                    \n",
    "        return scores, test_imgs, recon_imgs, gt_list, gt_mask_list\n",
    "    \n",
    "    def test(self, evaluated_objects, pro_integration_limit=0.3):\n",
    "        \n",
    "        assert 0.0 < pro_integration_limit <= 1.0\n",
    "        root_anomaly_map_dir=f'anomaly_maps/{self.save_name}'\n",
    "        output_dir=f'metrics/{self.save_name}'\n",
    "        evaluation_dict = dict()\n",
    "        # Keep track of the mean performance measures.\n",
    "        au_pros = []\n",
    "        au_rocs = []\n",
    "        \n",
    "        p_acs = []\n",
    "        p_prs = []\n",
    "        p_res = []\n",
    "        p_f1s = []\n",
    "        i_acs = []\n",
    "        i_prs = []\n",
    "        i_res = []\n",
    "        i_f1s = []\n",
    "        \n",
    "\n",
    "        # Evaluate each dataset object separately.\n",
    "        for obj in evaluated_objects:\n",
    "            print(f\"=== Evaluate {obj} ===\")\n",
    "            evaluation_dict[obj] = dict()\n",
    "            \n",
    "            test_dataset = MvtecADDataset(root_dir=f\"mvtec_anomaly_detection_{self.args.img_size}\", split=\"test\", img_size=self.args.img_size, object_names=[obj])\n",
    "            test_loader = DataLoader(test_dataset, batch_size=self.args.batch_size, shuffle=False)\n",
    "            scores, test_imgs, recon_imgs, gt_list, gt_mask_list = self._test(test_loader=test_loader, root_anomaly_map_dir=root_anomaly_map_dir)\n",
    "            scores = np.asarray(scores)\n",
    "\n",
    "            # Calculate the PRO and ROC curves.\n",
    "            au_pro, au_roc, pro_curve, roc_curve, pixel_level_metrics, image_level_metrics = \\\n",
    "                calculate_metrics(\n",
    "                    np.asanyarray(gt_mask_list).squeeze(axis=1),\n",
    "                    scores,\n",
    "                    pro_integration_limit)\n",
    "                \n",
    "            threshold = pixel_level_metrics['threshold']\n",
    "            save_dir = f'metrics/{self.save_name}/pictures_{obj}'\n",
    "            os.makedirs(save_dir, exist_ok=True)\n",
    "            plot_fig(test_img=test_imgs, recon_imgs=recon_imgs, scores=scores, gts=gt_mask_list, threshold=threshold, save_dir=save_dir)\n",
    "            \n",
    "            evaluation_dict[obj]['au_pro'] = au_pro\n",
    "            evaluation_dict[obj]['classification_au_roc'] = au_roc\n",
    "            evaluation_dict[obj]['pixel_level_accuracy'] = pixel_level_metrics['accuracy']\n",
    "            evaluation_dict[obj]['pixel_level_precision'] = pixel_level_metrics['precision']\n",
    "            evaluation_dict[obj]['pixel_level_recall'] = pixel_level_metrics['recall']\n",
    "            evaluation_dict[obj]['pixel_level_f1_score'] = pixel_level_metrics['f1']\n",
    "            evaluation_dict[obj]['image_level_accuracy'] = image_level_metrics['accuracy']\n",
    "            evaluation_dict[obj]['image_level_precision'] = image_level_metrics['precision']\n",
    "            evaluation_dict[obj]['image_level_recall'] = image_level_metrics['recall']\n",
    "            evaluation_dict[obj]['image_level_f1_score'] = image_level_metrics['f1']\n",
    "            \n",
    "\n",
    "            evaluation_dict[obj]['classification_roc_curve_fpr'] = roc_curve[0]\n",
    "            evaluation_dict[obj]['classification_roc_curve_tpr'] = roc_curve[1]\n",
    "\n",
    "            # Keep track of the mean performance measures.\n",
    "            au_pros.append(au_pro)\n",
    "            au_rocs.append(au_roc)\n",
    "            p_acs.append(pixel_level_metrics['accuracy'])\n",
    "            p_prs.append(pixel_level_metrics['precision'])\n",
    "            p_res.append(pixel_level_metrics['recall'])\n",
    "            p_f1s.append(pixel_level_metrics['f1'])\n",
    "            i_acs.append(image_level_metrics['accuracy'])\n",
    "            i_prs.append(image_level_metrics['precision'])\n",
    "            i_res.append(image_level_metrics['recall'])\n",
    "            i_f1s.append(image_level_metrics['f1'])\n",
    "\n",
    "            print('\\n')\n",
    "\n",
    "        # Compute the mean of the performance measures.\n",
    "        evaluation_dict['mean_au_pro'] = np.mean(au_pros).item()\n",
    "        evaluation_dict['mean_classification_au_roc'] = np.mean(au_rocs).item()\n",
    "        \n",
    "        evaluation_dict['mean_pixel_level_accuracy'] = np.mean(p_acs).item()\n",
    "        evaluation_dict['mean_pixel_level_precision'] = np.mean(p_prs).item()\n",
    "        evaluation_dict['mean_pixel_level_recall'] = np.mean(p_res).item()\n",
    "        evaluation_dict['mean_pixel_level_f1_score'] = np.mean(p_f1s).item()\n",
    "        evaluation_dict['mean_image_level_accuracy'] = np.mean(i_acs).item()\n",
    "        evaluation_dict['mean_image_level_precision'] = np.mean(i_prs).item()\n",
    "        evaluation_dict['mean_image_level_recall'] = np.mean(i_res).item()\n",
    "        evaluation_dict['mean_image_level_f1_score'] = np.mean(i_f1s).item()\n",
    "\n",
    "        # If required, write evaluation metrics to drive.\n",
    "        if output_dir is not None:\n",
    "            makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            with open(path.join(output_dir, 'metrics.json'), 'w') as file:\n",
    "                json.dump(evaluation_dict, file, indent=4)\n",
    "\n",
    "            print(f\"Wrote metrics to {path.join(output_dir, 'metrics.json')}\")\n",
    "    \n",
    "    def load_model(self, state_dict_path):\n",
    "        self.model.load_state_dict(torch.load(state_dict_path, weights_only=True))\n",
    "    \n",
    "    def save_snapshot(self, x, x2, save_dir, save_dir2):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            x_fake_list = x\n",
    "            recon, _ = self.model(x)\n",
    "            x_concat = torch.cat((x_fake_list, recon), dim=3)\n",
    "            save_image((x_concat.data.cpu()), save_dir, nrow=1, padding=0)\n",
    "            print(('Saved real and fake images into {}...'.format(save_dir)))\n",
    "\n",
    "            x_fake_list = x2\n",
    "            recon, _ = self.model(x2)\n",
    "            x_concat = torch.cat((x_fake_list, recon), dim=3)\n",
    "            save_image((x_concat.data.cpu()), save_dir2, nrow=1, padding=0)\n",
    "            print(('Saved real and fake images into {}...'.format(save_dir2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leejinhyeok/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "/home/leejinhyeok/.local/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:60: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "sys.argv = ['script_name', '--epochs', '100',]\n",
    "parser = argparse.ArgumentParser(description='Process some integers.')\n",
    "parser.add_argument('--prefix', type=str, default=None)\n",
    "parser.add_argument('--epochs', type=int, default=100)\n",
    "parser.add_argument('--lr', type=float, default=1e-3)\n",
    "parser.add_argument('--batch_size', type=int, default=16)\n",
    "parser.add_argument('--img_size', type=int, default=224)\n",
    "parser.add_argument('--alpha', type=float, default=1.0)\n",
    "parser.add_argument('--beta', type=float, default=1.0)\n",
    "parser.add_argument('--gamma', type=float, default=1.0)\n",
    "parser.add_argument('--delta', type=float, default=0.0)\n",
    "parser.add_argument('--seed', type=int, default=None, help='manual seed')\n",
    "args = parser.parse_args()\n",
    "\n",
    "exp = SimpleAD(args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Evaluate bottle ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e767c0c8fa77434da2798d74a04e7152",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 4164608 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.5983889377326753\n",
      "Threshold: 0.1865\n",
      "Pixel-level Accuracy: 0.9074\n",
      "Pixel-level Precision: 0.2911\n",
      "Pixel-level Recall: 0.4204\n",
      "Pixel-level F1 Score: 0.3440\n",
      "Image-level classification AU-ROC: 0.9285714285714286\n",
      "Threshold: 0.4626\n",
      "Image-level Accuracy: 0.5783\n",
      "Image-level Precision: 1.0000\n",
      "Image-level Recall: 0.4444\n",
      "Image-level F1 Score: 0.6154\n",
      "\n",
      "\n",
      "=== Evaluate cable ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1469d0e6475e49139e96dc2d36336c03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 7526400 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.4097633831878087\n",
      "Threshold: 0.2929\n",
      "Pixel-level Accuracy: 0.9428\n",
      "Pixel-level Precision: 0.1910\n",
      "Pixel-level Recall: 0.3062\n",
      "Pixel-level F1 Score: 0.2353\n",
      "Image-level classification AU-ROC: 0.7358508245877061\n",
      "Threshold: 0.3888\n",
      "Image-level Accuracy: 0.6533\n",
      "Image-level Precision: 0.7500\n",
      "Image-level Recall: 0.6522\n",
      "Image-level F1 Score: 0.6977\n",
      "\n",
      "\n",
      "=== Evaluate capsule ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4560594953149f399665b906c74efe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 6623232 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.6687455951885157\n",
      "Threshold: 0.2073\n",
      "Pixel-level Accuracy: 0.9832\n",
      "Pixel-level Precision: 0.1992\n",
      "Pixel-level Recall: 0.2776\n",
      "Pixel-level F1 Score: 0.2319\n",
      "Image-level classification AU-ROC: 0.566414040686079\n",
      "Threshold: 0.2310\n",
      "Image-level Accuracy: 0.5682\n",
      "Image-level Precision: 0.8611\n",
      "Image-level Recall: 0.5688\n",
      "Image-level F1 Score: 0.6851\n",
      "\n",
      "\n",
      "=== Evaluate carpet ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc56925a06b442bc9bab19876539b270",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 5870592 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.7098573915978729\n",
      "Threshold: 0.1732\n",
      "Pixel-level Accuracy: 0.9541\n",
      "Pixel-level Precision: 0.2039\n",
      "Pixel-level Recall: 0.6408\n",
      "Pixel-level F1 Score: 0.3094\n",
      "Image-level classification AU-ROC: 0.8611556982343499\n",
      "Threshold: 0.2349\n",
      "Image-level Accuracy: 0.6496\n",
      "Image-level Precision: 1.0000\n",
      "Image-level Recall: 0.5393\n",
      "Image-level F1 Score: 0.7007\n",
      "\n",
      "\n",
      "=== Evaluate grid ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c10d63dc0c942089c1cee48c180001d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 3913728 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.2909166868862982\n",
      "Threshold: 0.2366\n",
      "Pixel-level Accuracy: 0.9829\n",
      "Pixel-level Precision: 0.0233\n",
      "Pixel-level Recall: 0.0361\n",
      "Pixel-level F1 Score: 0.0283\n",
      "Image-level classification AU-ROC: 0.7167919799498746\n",
      "Threshold: 0.2824\n",
      "Image-level Accuracy: 0.3590\n",
      "Image-level Precision: 0.8889\n",
      "Image-level Recall: 0.1404\n",
      "Image-level F1 Score: 0.2424\n",
      "\n",
      "\n",
      "=== Evaluate hazelnut ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6fdb4c6de42408d9b0a0f340ada7cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 5519360 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.7818229292800275\n",
      "Threshold: 0.2937\n",
      "Pixel-level Accuracy: 0.9812\n",
      "Pixel-level Precision: 0.5661\n",
      "Pixel-level Recall: 0.5107\n",
      "Pixel-level F1 Score: 0.5370\n",
      "Image-level classification AU-ROC: 0.827857142857143\n",
      "Threshold: 0.2955\n",
      "Image-level Accuracy: 0.7273\n",
      "Image-level Precision: 0.8125\n",
      "Image-level Recall: 0.7429\n",
      "Image-level F1 Score: 0.7761\n",
      "\n",
      "\n",
      "=== Evaluate leather ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81992b506f7b426a80fd83b589bb1bf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 6221824 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.9007491215650754\n",
      "Threshold: 0.3000\n",
      "Pixel-level Accuracy: 0.9905\n",
      "Pixel-level Precision: 0.3207\n",
      "Pixel-level Recall: 0.4172\n",
      "Pixel-level F1 Score: 0.3626\n",
      "Image-level classification AU-ROC: 0.9402173913043478\n",
      "Threshold: 0.3613\n",
      "Image-level Accuracy: 0.5323\n",
      "Image-level Precision: 1.0000\n",
      "Image-level Recall: 0.3696\n",
      "Image-level F1 Score: 0.5397\n",
      "\n",
      "\n",
      "=== Evaluate metal_nut ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5224d412cc74a79a34cd5eded6ab36e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 5770240 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.515807654773548\n",
      "Threshold: 0.1885\n",
      "Pixel-level Accuracy: 0.8641\n",
      "Pixel-level Precision: 0.4444\n",
      "Pixel-level Recall: 0.6377\n",
      "Pixel-level F1 Score: 0.5238\n",
      "Image-level classification AU-ROC: 0.7385141739980449\n",
      "Threshold: 0.4124\n",
      "Image-level Accuracy: 0.5217\n",
      "Image-level Precision: 0.9318\n",
      "Image-level Recall: 0.4409\n",
      "Image-level F1 Score: 0.5985\n",
      "\n",
      "\n",
      "=== Evaluate pill ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ae5a28f8dc44d88a1dbccaeb5b4ac06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 8379392 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.7721693929672467\n",
      "Threshold: 0.1481\n",
      "Pixel-level Accuracy: 0.9610\n",
      "Pixel-level Precision: 0.4206\n",
      "Pixel-level Recall: 0.4253\n",
      "Pixel-level F1 Score: 0.4229\n",
      "Image-level classification AU-ROC: 0.640480087288598\n",
      "Threshold: 0.2459\n",
      "Image-level Accuracy: 0.5808\n",
      "Image-level Precision: 0.8989\n",
      "Image-level Recall: 0.5674\n",
      "Image-level F1 Score: 0.6957\n",
      "\n",
      "\n",
      "=== Evaluate screw ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d671f072e03b483d9a9ef4595a674922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 8028160 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.8262349472277485\n",
      "Threshold: 0.2026\n",
      "Pixel-level Accuracy: 0.9809\n",
      "Pixel-level Precision: 0.0393\n",
      "Pixel-level Recall: 0.2844\n",
      "Pixel-level F1 Score: 0.0691\n",
      "Image-level classification AU-ROC: 0.6394753023160483\n",
      "Threshold: 0.3184\n",
      "Image-level Accuracy: 0.3812\n",
      "Image-level Precision: 0.8125\n",
      "Image-level Recall: 0.2185\n",
      "Image-level F1 Score: 0.3444\n",
      "\n",
      "\n",
      "=== Evaluate tile ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eee41df5d56d40eeb5d93f402e40144d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 5870592 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.515360239189716\n",
      "Threshold: 0.1771\n",
      "Pixel-level Accuracy: 0.8528\n",
      "Pixel-level Precision: 0.2080\n",
      "Pixel-level Recall: 0.3889\n",
      "Pixel-level F1 Score: 0.2711\n",
      "Image-level classification AU-ROC: 0.9502164502164502\n",
      "Threshold: 0.3368\n",
      "Image-level Accuracy: 0.6496\n",
      "Image-level Precision: 1.0000\n",
      "Image-level Recall: 0.5119\n",
      "Image-level F1 Score: 0.6772\n",
      "\n",
      "\n",
      "=== Evaluate toothbrush ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79888631bec043119393bc6a2fd2a791",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 2107392 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.8118730503070536\n",
      "Threshold: 0.1467\n",
      "Pixel-level Accuracy: 0.9672\n",
      "Pixel-level Precision: 0.2801\n",
      "Pixel-level Recall: 0.7233\n",
      "Pixel-level F1 Score: 0.4039\n",
      "Image-level classification AU-ROC: 0.9944444444444445\n",
      "Threshold: 0.3268\n",
      "Image-level Accuracy: 0.5952\n",
      "Image-level Precision: 1.0000\n",
      "Image-level Recall: 0.4333\n",
      "Image-level F1 Score: 0.6047\n",
      "\n",
      "\n",
      "=== Evaluate transistor ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "189e28a60dc34bc491983e64995dbc5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 5017600 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.495110008199993\n",
      "Threshold: 0.2666\n",
      "Pixel-level Accuracy: 0.9041\n",
      "Pixel-level Precision: 0.2426\n",
      "Pixel-level Recall: 0.4717\n",
      "Pixel-level F1 Score: 0.3204\n",
      "Image-level classification AU-ROC: 0.8366666666666667\n",
      "Threshold: 0.3432\n",
      "Image-level Accuracy: 0.7200\n",
      "Image-level Precision: 0.6111\n",
      "Image-level Recall: 0.8250\n",
      "Image-level F1 Score: 0.7021\n",
      "\n",
      "\n",
      "=== Evaluate wood ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1cb77c13c1d4b189ab0f8c3f78442cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 3963904 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.45907605542078955\n",
      "Threshold: 0.1915\n",
      "Pixel-level Accuracy: 0.9360\n",
      "Pixel-level Precision: 0.2174\n",
      "Pixel-level Recall: 0.2520\n",
      "Pixel-level F1 Score: 0.2334\n",
      "Image-level classification AU-ROC: 0.9298245614035088\n",
      "Threshold: 0.2729\n",
      "Image-level Accuracy: 0.5570\n",
      "Image-level Precision: 1.0000\n",
      "Image-level Recall: 0.4167\n",
      "Image-level F1 Score: 0.5882\n",
      "\n",
      "\n",
      "=== Evaluate zipper ===\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40e14a5ee60e474885ba3a76369179de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute PRO curve...\n",
      "Sort 7576576 anomaly scores...\n",
      "AU-PRO (FPR limit: 0.3): 0.34261895809522647\n",
      "Threshold: 0.0711\n",
      "Pixel-level Accuracy: 0.7266\n",
      "Pixel-level Precision: 0.0506\n",
      "Pixel-level Recall: 0.6877\n",
      "Pixel-level F1 Score: 0.0943\n",
      "Image-level classification AU-ROC: 0.6932773109243697\n",
      "Threshold: 0.3019\n",
      "Image-level Accuracy: 0.6291\n",
      "Image-level Precision: 0.9091\n",
      "Image-level Recall: 0.5882\n",
      "Image-level F1 Score: 0.7143\n",
      "\n",
      "\n",
      "Wrote metrics to metrics/SimpleSAE_224/metrics.json\n"
     ]
    }
   ],
   "source": [
    "OBJECT_NAMES = ['bottle', 'cable', 'capsule', 'carpet', 'grid',\n",
    "                'hazelnut', 'leather', 'metal_nut', 'pill', 'screw',\n",
    "                'tile', 'toothbrush', 'transistor', 'wood', 'zipper']\n",
    "exp.load_model('save/SimpleSAE_224')\n",
    "exp.test(evaluated_objects=OBJECT_NAMES, pro_integration_limit=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
