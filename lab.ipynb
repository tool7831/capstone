{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile as tiff\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tiff.imread('anomaly_maps/EfficientNetV2SAutoencoder_224/bottle/test/broken_large/010.tiff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.03946198e-04, 2.00474780e-04, 1.93531931e-04, ...,\n",
       "        1.07035594e-04, 1.09928449e-04, 1.11374873e-04],\n",
       "       [2.00185488e-04, 1.96714071e-04, 1.90060498e-04, ...,\n",
       "        1.05589170e-04, 1.08482018e-04, 1.09928449e-04],\n",
       "       [1.92953361e-04, 1.89481929e-04, 1.83117649e-04, ...,\n",
       "        1.02696315e-04, 1.05299885e-04, 1.07035594e-04],\n",
       "       ...,\n",
       "       [6.07499305e-06, 5.78570780e-06, 5.49642255e-06, ...,\n",
       "        9.45963257e-05, 9.83570353e-05, 1.00382029e-04],\n",
       "       [6.07499305e-06, 6.07499305e-06, 5.78570780e-06, ...,\n",
       "        9.83570353e-05, 1.02407030e-04, 1.04142739e-04],\n",
       "       [6.36427876e-06, 6.07499305e-06, 5.78570780e-06, ...,\n",
       "        1.00382029e-04, 1.04142739e-04, 1.06167739e-04]], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nom_img = (img - np.min(img))/(np.max(img) - np.min(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "446"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(img > 0.7)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('mvtec_anomaly_detection_224/bottle/ground_truth/broken_large/000_mask.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_img = np.asarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np_img == 1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leejinhyeok/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n",
      "Downloading: \"https://download.pytorch.org/models/swin_b-68c6b09e.pth\" to /home/leejinhyeok/.cache/torch/hub/checkpoints/swin_b-68c6b09e.pth\n",
      "100%|██████████| 335M/335M [00:30<00:00, 11.4MB/s] \n"
     ]
    }
   ],
   "source": [
    "from torchvision.models import swin_b, Swin_B_Weights\n",
    "from torchsummary import summary\n",
    "\n",
    "model = swin_b(Swin_B_Weights.DEFAULT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 56, 56]           6,272\n",
      "           Permute-2          [-1, 56, 56, 128]               0\n",
      "         LayerNorm-3          [-1, 56, 56, 128]             256\n",
      "         LayerNorm-4          [-1, 56, 56, 128]             256\n",
      "ShiftedWindowAttention-5          [-1, 56, 56, 128]               0\n",
      "   StochasticDepth-6          [-1, 56, 56, 128]               0\n",
      "         LayerNorm-7          [-1, 56, 56, 128]             256\n",
      "            Linear-8          [-1, 56, 56, 512]          66,048\n",
      "              GELU-9          [-1, 56, 56, 512]               0\n",
      "          Dropout-10          [-1, 56, 56, 512]               0\n",
      "           Linear-11          [-1, 56, 56, 128]          65,664\n",
      "          Dropout-12          [-1, 56, 56, 128]               0\n",
      "  StochasticDepth-13          [-1, 56, 56, 128]               0\n",
      "SwinTransformerBlock-14          [-1, 56, 56, 128]               0\n",
      "        LayerNorm-15          [-1, 56, 56, 128]             256\n",
      "ShiftedWindowAttention-16          [-1, 56, 56, 128]               0\n",
      "  StochasticDepth-17          [-1, 56, 56, 128]               0\n",
      "        LayerNorm-18          [-1, 56, 56, 128]             256\n",
      "           Linear-19          [-1, 56, 56, 512]          66,048\n",
      "             GELU-20          [-1, 56, 56, 512]               0\n",
      "          Dropout-21          [-1, 56, 56, 512]               0\n",
      "           Linear-22          [-1, 56, 56, 128]          65,664\n",
      "          Dropout-23          [-1, 56, 56, 128]               0\n",
      "  StochasticDepth-24          [-1, 56, 56, 128]               0\n",
      "SwinTransformerBlock-25          [-1, 56, 56, 128]               0\n",
      "        LayerNorm-26          [-1, 28, 28, 512]           1,024\n",
      "           Linear-27          [-1, 28, 28, 256]         131,072\n",
      "     PatchMerging-28          [-1, 28, 28, 256]               0\n",
      "        LayerNorm-29          [-1, 28, 28, 256]             512\n",
      "ShiftedWindowAttention-30          [-1, 28, 28, 256]               0\n",
      "  StochasticDepth-31          [-1, 28, 28, 256]               0\n",
      "        LayerNorm-32          [-1, 28, 28, 256]             512\n",
      "           Linear-33         [-1, 28, 28, 1024]         263,168\n",
      "             GELU-34         [-1, 28, 28, 1024]               0\n",
      "          Dropout-35         [-1, 28, 28, 1024]               0\n",
      "           Linear-36          [-1, 28, 28, 256]         262,400\n",
      "          Dropout-37          [-1, 28, 28, 256]               0\n",
      "  StochasticDepth-38          [-1, 28, 28, 256]               0\n",
      "SwinTransformerBlock-39          [-1, 28, 28, 256]               0\n",
      "        LayerNorm-40          [-1, 28, 28, 256]             512\n",
      "ShiftedWindowAttention-41          [-1, 28, 28, 256]               0\n",
      "  StochasticDepth-42          [-1, 28, 28, 256]               0\n",
      "        LayerNorm-43          [-1, 28, 28, 256]             512\n",
      "           Linear-44         [-1, 28, 28, 1024]         263,168\n",
      "             GELU-45         [-1, 28, 28, 1024]               0\n",
      "          Dropout-46         [-1, 28, 28, 1024]               0\n",
      "           Linear-47          [-1, 28, 28, 256]         262,400\n",
      "          Dropout-48          [-1, 28, 28, 256]               0\n",
      "  StochasticDepth-49          [-1, 28, 28, 256]               0\n",
      "SwinTransformerBlock-50          [-1, 28, 28, 256]               0\n",
      "        LayerNorm-51         [-1, 14, 14, 1024]           2,048\n",
      "           Linear-52          [-1, 14, 14, 512]         524,288\n",
      "     PatchMerging-53          [-1, 14, 14, 512]               0\n",
      "        LayerNorm-54          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-55          [-1, 14, 14, 512]               0\n",
      "  StochasticDepth-56          [-1, 14, 14, 512]               0\n",
      "        LayerNorm-57          [-1, 14, 14, 512]           1,024\n",
      "           Linear-58         [-1, 14, 14, 2048]       1,050,624\n",
      "             GELU-59         [-1, 14, 14, 2048]               0\n",
      "          Dropout-60         [-1, 14, 14, 2048]               0\n",
      "           Linear-61          [-1, 14, 14, 512]       1,049,088\n",
      "          Dropout-62          [-1, 14, 14, 512]               0\n",
      "  StochasticDepth-63          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-64          [-1, 14, 14, 512]               0\n",
      "        LayerNorm-65          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-66          [-1, 14, 14, 512]               0\n",
      "  StochasticDepth-67          [-1, 14, 14, 512]               0\n",
      "        LayerNorm-68          [-1, 14, 14, 512]           1,024\n",
      "           Linear-69         [-1, 14, 14, 2048]       1,050,624\n",
      "             GELU-70         [-1, 14, 14, 2048]               0\n",
      "          Dropout-71         [-1, 14, 14, 2048]               0\n",
      "           Linear-72          [-1, 14, 14, 512]       1,049,088\n",
      "          Dropout-73          [-1, 14, 14, 512]               0\n",
      "  StochasticDepth-74          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-75          [-1, 14, 14, 512]               0\n",
      "        LayerNorm-76          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-77          [-1, 14, 14, 512]               0\n",
      "  StochasticDepth-78          [-1, 14, 14, 512]               0\n",
      "        LayerNorm-79          [-1, 14, 14, 512]           1,024\n",
      "           Linear-80         [-1, 14, 14, 2048]       1,050,624\n",
      "             GELU-81         [-1, 14, 14, 2048]               0\n",
      "          Dropout-82         [-1, 14, 14, 2048]               0\n",
      "           Linear-83          [-1, 14, 14, 512]       1,049,088\n",
      "          Dropout-84          [-1, 14, 14, 512]               0\n",
      "  StochasticDepth-85          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-86          [-1, 14, 14, 512]               0\n",
      "        LayerNorm-87          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-88          [-1, 14, 14, 512]               0\n",
      "  StochasticDepth-89          [-1, 14, 14, 512]               0\n",
      "        LayerNorm-90          [-1, 14, 14, 512]           1,024\n",
      "           Linear-91         [-1, 14, 14, 2048]       1,050,624\n",
      "             GELU-92         [-1, 14, 14, 2048]               0\n",
      "          Dropout-93         [-1, 14, 14, 2048]               0\n",
      "           Linear-94          [-1, 14, 14, 512]       1,049,088\n",
      "          Dropout-95          [-1, 14, 14, 512]               0\n",
      "  StochasticDepth-96          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-97          [-1, 14, 14, 512]               0\n",
      "        LayerNorm-98          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-99          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-100          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-101          [-1, 14, 14, 512]           1,024\n",
      "          Linear-102         [-1, 14, 14, 2048]       1,050,624\n",
      "            GELU-103         [-1, 14, 14, 2048]               0\n",
      "         Dropout-104         [-1, 14, 14, 2048]               0\n",
      "          Linear-105          [-1, 14, 14, 512]       1,049,088\n",
      "         Dropout-106          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-107          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-108          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-109          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-110          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-111          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-112          [-1, 14, 14, 512]           1,024\n",
      "          Linear-113         [-1, 14, 14, 2048]       1,050,624\n",
      "            GELU-114         [-1, 14, 14, 2048]               0\n",
      "         Dropout-115         [-1, 14, 14, 2048]               0\n",
      "          Linear-116          [-1, 14, 14, 512]       1,049,088\n",
      "         Dropout-117          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-118          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-119          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-120          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-121          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-122          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-123          [-1, 14, 14, 512]           1,024\n",
      "          Linear-124         [-1, 14, 14, 2048]       1,050,624\n",
      "            GELU-125         [-1, 14, 14, 2048]               0\n",
      "         Dropout-126         [-1, 14, 14, 2048]               0\n",
      "          Linear-127          [-1, 14, 14, 512]       1,049,088\n",
      "         Dropout-128          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-129          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-130          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-131          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-132          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-133          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-134          [-1, 14, 14, 512]           1,024\n",
      "          Linear-135         [-1, 14, 14, 2048]       1,050,624\n",
      "            GELU-136         [-1, 14, 14, 2048]               0\n",
      "         Dropout-137         [-1, 14, 14, 2048]               0\n",
      "          Linear-138          [-1, 14, 14, 512]       1,049,088\n",
      "         Dropout-139          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-140          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-141          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-142          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-143          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-144          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-145          [-1, 14, 14, 512]           1,024\n",
      "          Linear-146         [-1, 14, 14, 2048]       1,050,624\n",
      "            GELU-147         [-1, 14, 14, 2048]               0\n",
      "         Dropout-148         [-1, 14, 14, 2048]               0\n",
      "          Linear-149          [-1, 14, 14, 512]       1,049,088\n",
      "         Dropout-150          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-151          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-152          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-153          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-154          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-155          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-156          [-1, 14, 14, 512]           1,024\n",
      "          Linear-157         [-1, 14, 14, 2048]       1,050,624\n",
      "            GELU-158         [-1, 14, 14, 2048]               0\n",
      "         Dropout-159         [-1, 14, 14, 2048]               0\n",
      "          Linear-160          [-1, 14, 14, 512]       1,049,088\n",
      "         Dropout-161          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-162          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-163          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-164          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-165          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-166          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-167          [-1, 14, 14, 512]           1,024\n",
      "          Linear-168         [-1, 14, 14, 2048]       1,050,624\n",
      "            GELU-169         [-1, 14, 14, 2048]               0\n",
      "         Dropout-170         [-1, 14, 14, 2048]               0\n",
      "          Linear-171          [-1, 14, 14, 512]       1,049,088\n",
      "         Dropout-172          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-173          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-174          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-175          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-176          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-177          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-178          [-1, 14, 14, 512]           1,024\n",
      "          Linear-179         [-1, 14, 14, 2048]       1,050,624\n",
      "            GELU-180         [-1, 14, 14, 2048]               0\n",
      "         Dropout-181         [-1, 14, 14, 2048]               0\n",
      "          Linear-182          [-1, 14, 14, 512]       1,049,088\n",
      "         Dropout-183          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-184          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-185          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-186          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-187          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-188          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-189          [-1, 14, 14, 512]           1,024\n",
      "          Linear-190         [-1, 14, 14, 2048]       1,050,624\n",
      "            GELU-191         [-1, 14, 14, 2048]               0\n",
      "         Dropout-192         [-1, 14, 14, 2048]               0\n",
      "          Linear-193          [-1, 14, 14, 512]       1,049,088\n",
      "         Dropout-194          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-195          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-196          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-197          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-198          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-199          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-200          [-1, 14, 14, 512]           1,024\n",
      "          Linear-201         [-1, 14, 14, 2048]       1,050,624\n",
      "            GELU-202         [-1, 14, 14, 2048]               0\n",
      "         Dropout-203         [-1, 14, 14, 2048]               0\n",
      "          Linear-204          [-1, 14, 14, 512]       1,049,088\n",
      "         Dropout-205          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-206          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-207          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-208          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-209          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-210          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-211          [-1, 14, 14, 512]           1,024\n",
      "          Linear-212         [-1, 14, 14, 2048]       1,050,624\n",
      "            GELU-213         [-1, 14, 14, 2048]               0\n",
      "         Dropout-214         [-1, 14, 14, 2048]               0\n",
      "          Linear-215          [-1, 14, 14, 512]       1,049,088\n",
      "         Dropout-216          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-217          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-218          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-219          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-220          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-221          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-222          [-1, 14, 14, 512]           1,024\n",
      "          Linear-223         [-1, 14, 14, 2048]       1,050,624\n",
      "            GELU-224         [-1, 14, 14, 2048]               0\n",
      "         Dropout-225         [-1, 14, 14, 2048]               0\n",
      "          Linear-226          [-1, 14, 14, 512]       1,049,088\n",
      "         Dropout-227          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-228          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-229          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-230          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-231          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-232          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-233          [-1, 14, 14, 512]           1,024\n",
      "          Linear-234         [-1, 14, 14, 2048]       1,050,624\n",
      "            GELU-235         [-1, 14, 14, 2048]               0\n",
      "         Dropout-236         [-1, 14, 14, 2048]               0\n",
      "          Linear-237          [-1, 14, 14, 512]       1,049,088\n",
      "         Dropout-238          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-239          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-240          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-241          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-242          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-243          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-244          [-1, 14, 14, 512]           1,024\n",
      "          Linear-245         [-1, 14, 14, 2048]       1,050,624\n",
      "            GELU-246         [-1, 14, 14, 2048]               0\n",
      "         Dropout-247         [-1, 14, 14, 2048]               0\n",
      "          Linear-248          [-1, 14, 14, 512]       1,049,088\n",
      "         Dropout-249          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-250          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-251          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-252           [-1, 7, 7, 2048]           4,096\n",
      "          Linear-253           [-1, 7, 7, 1024]       2,097,152\n",
      "    PatchMerging-254           [-1, 7, 7, 1024]               0\n",
      "       LayerNorm-255           [-1, 7, 7, 1024]           2,048\n",
      "ShiftedWindowAttention-256           [-1, 7, 7, 1024]               0\n",
      " StochasticDepth-257           [-1, 7, 7, 1024]               0\n",
      "       LayerNorm-258           [-1, 7, 7, 1024]           2,048\n",
      "          Linear-259           [-1, 7, 7, 4096]       4,198,400\n",
      "            GELU-260           [-1, 7, 7, 4096]               0\n",
      "         Dropout-261           [-1, 7, 7, 4096]               0\n",
      "          Linear-262           [-1, 7, 7, 1024]       4,195,328\n",
      "         Dropout-263           [-1, 7, 7, 1024]               0\n",
      " StochasticDepth-264           [-1, 7, 7, 1024]               0\n",
      "SwinTransformerBlock-265           [-1, 7, 7, 1024]               0\n",
      "       LayerNorm-266           [-1, 7, 7, 1024]           2,048\n",
      "ShiftedWindowAttention-267           [-1, 7, 7, 1024]               0\n",
      " StochasticDepth-268           [-1, 7, 7, 1024]               0\n",
      "       LayerNorm-269           [-1, 7, 7, 1024]           2,048\n",
      "          Linear-270           [-1, 7, 7, 4096]       4,198,400\n",
      "            GELU-271           [-1, 7, 7, 4096]               0\n",
      "         Dropout-272           [-1, 7, 7, 4096]               0\n",
      "          Linear-273           [-1, 7, 7, 1024]       4,195,328\n",
      "         Dropout-274           [-1, 7, 7, 1024]               0\n",
      " StochasticDepth-275           [-1, 7, 7, 1024]               0\n",
      "SwinTransformerBlock-276           [-1, 7, 7, 1024]               0\n",
      "================================================================\n",
      "Total params: 58,711,168\n",
      "Trainable params: 58,711,168\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 494.59\n",
      "Params size (MB): 223.97\n",
      "Estimated Total Size (MB): 719.13\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model.features, (3,224,224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=4, stride=2, padding=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        def conv_block(in_channels, out_channels):\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "            \n",
    "        layers = [conv_block(channels[i], channels[i + 1]) for i in range(len(channels) - 1)]\n",
    "        layers.append(nn.Sigmoid())\n",
    "        self.decoder = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "class SwinT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SwinT, self).__init__()\n",
    "        self.encoder = swin_b(Swin_B_Weights.DEFAULT).features\n",
    "        \n",
    "        self.decoder = Decoder([1024, 512, 256, 128, 64, 3])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = torch.permute(x,(0,2,3,1))\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leejinhyeok/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:135: UserWarning: Using 'weights' as positional parameter(s) is deprecated since 0.13 and may be removed in the future. Please use keyword parameter(s) instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = SwinT()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwinT(\n",
       "  (encoder): Sequential(\n",
       "    (0): Sequential(\n",
       "      (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "      (1): Permute()\n",
       "      (2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (1): Sequential(\n",
       "      (0): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=128, out_features=384, bias=True)\n",
       "          (proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.021739130434782608, mode=row)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=512, out_features=128, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): PatchMerging(\n",
       "      (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (3): Sequential(\n",
       "      (0): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.043478260869565216, mode=row)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.06521739130434782, mode=row)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (4): PatchMerging(\n",
       "      (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "      (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (5): Sequential(\n",
       "      (0): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.08695652173913043, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.10869565217391304, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.13043478260869565, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.15217391304347827, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.17391304347826086, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.1956521739130435, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.21739130434782608, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.2391304347826087, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.2608695652173913, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.2826086956521739, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.30434782608695654, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.32608695652173914, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (12): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.34782608695652173, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (13): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.3695652173913043, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (14): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.391304347826087, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (15): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.41304347826086957, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (16): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.43478260869565216, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (17): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (proj): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.45652173913043476, mode=row)\n",
       "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (6): PatchMerging(\n",
       "      (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "      (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (7): Sequential(\n",
       "      (0): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.4782608695652174, mode=row)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): SwinTransformerBlock(\n",
       "        (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): ShiftedWindowAttention(\n",
       "          (qkv): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "          (proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        )\n",
       "        (stochastic_depth): StochasticDepth(p=0.5, mode=row)\n",
       "        (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (0): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (decoder): Sequential(\n",
       "      (0): Sequential(\n",
       "        (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (0): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (4): Sequential(\n",
       "        (0): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "        (1): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (5): Sigmoid()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [-1, 128, 56, 56]           6,272\n",
      "           Permute-2          [-1, 56, 56, 128]               0\n",
      "         LayerNorm-3          [-1, 56, 56, 128]             256\n",
      "         LayerNorm-4          [-1, 56, 56, 128]             256\n",
      "ShiftedWindowAttention-5          [-1, 56, 56, 128]               0\n",
      "   StochasticDepth-6          [-1, 56, 56, 128]               0\n",
      "         LayerNorm-7          [-1, 56, 56, 128]             256\n",
      "            Linear-8          [-1, 56, 56, 512]          66,048\n",
      "              GELU-9          [-1, 56, 56, 512]               0\n",
      "          Dropout-10          [-1, 56, 56, 512]               0\n",
      "           Linear-11          [-1, 56, 56, 128]          65,664\n",
      "          Dropout-12          [-1, 56, 56, 128]               0\n",
      "  StochasticDepth-13          [-1, 56, 56, 128]               0\n",
      "SwinTransformerBlock-14          [-1, 56, 56, 128]               0\n",
      "        LayerNorm-15          [-1, 56, 56, 128]             256\n",
      "ShiftedWindowAttention-16          [-1, 56, 56, 128]               0\n",
      "  StochasticDepth-17          [-1, 56, 56, 128]               0\n",
      "        LayerNorm-18          [-1, 56, 56, 128]             256\n",
      "           Linear-19          [-1, 56, 56, 512]          66,048\n",
      "             GELU-20          [-1, 56, 56, 512]               0\n",
      "          Dropout-21          [-1, 56, 56, 512]               0\n",
      "           Linear-22          [-1, 56, 56, 128]          65,664\n",
      "          Dropout-23          [-1, 56, 56, 128]               0\n",
      "  StochasticDepth-24          [-1, 56, 56, 128]               0\n",
      "SwinTransformerBlock-25          [-1, 56, 56, 128]               0\n",
      "        LayerNorm-26          [-1, 28, 28, 512]           1,024\n",
      "           Linear-27          [-1, 28, 28, 256]         131,072\n",
      "     PatchMerging-28          [-1, 28, 28, 256]               0\n",
      "        LayerNorm-29          [-1, 28, 28, 256]             512\n",
      "ShiftedWindowAttention-30          [-1, 28, 28, 256]               0\n",
      "  StochasticDepth-31          [-1, 28, 28, 256]               0\n",
      "        LayerNorm-32          [-1, 28, 28, 256]             512\n",
      "           Linear-33         [-1, 28, 28, 1024]         263,168\n",
      "             GELU-34         [-1, 28, 28, 1024]               0\n",
      "          Dropout-35         [-1, 28, 28, 1024]               0\n",
      "           Linear-36          [-1, 28, 28, 256]         262,400\n",
      "          Dropout-37          [-1, 28, 28, 256]               0\n",
      "  StochasticDepth-38          [-1, 28, 28, 256]               0\n",
      "SwinTransformerBlock-39          [-1, 28, 28, 256]               0\n",
      "        LayerNorm-40          [-1, 28, 28, 256]             512\n",
      "ShiftedWindowAttention-41          [-1, 28, 28, 256]               0\n",
      "  StochasticDepth-42          [-1, 28, 28, 256]               0\n",
      "        LayerNorm-43          [-1, 28, 28, 256]             512\n",
      "           Linear-44         [-1, 28, 28, 1024]         263,168\n",
      "             GELU-45         [-1, 28, 28, 1024]               0\n",
      "          Dropout-46         [-1, 28, 28, 1024]               0\n",
      "           Linear-47          [-1, 28, 28, 256]         262,400\n",
      "          Dropout-48          [-1, 28, 28, 256]               0\n",
      "  StochasticDepth-49          [-1, 28, 28, 256]               0\n",
      "SwinTransformerBlock-50          [-1, 28, 28, 256]               0\n",
      "        LayerNorm-51         [-1, 14, 14, 1024]           2,048\n",
      "           Linear-52          [-1, 14, 14, 512]         524,288\n",
      "     PatchMerging-53          [-1, 14, 14, 512]               0\n",
      "        LayerNorm-54          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-55          [-1, 14, 14, 512]               0\n",
      "  StochasticDepth-56          [-1, 14, 14, 512]               0\n",
      "        LayerNorm-57          [-1, 14, 14, 512]           1,024\n",
      "           Linear-58         [-1, 14, 14, 2048]       1,050,624\n",
      "             GELU-59         [-1, 14, 14, 2048]               0\n",
      "          Dropout-60         [-1, 14, 14, 2048]               0\n",
      "           Linear-61          [-1, 14, 14, 512]       1,049,088\n",
      "          Dropout-62          [-1, 14, 14, 512]               0\n",
      "  StochasticDepth-63          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-64          [-1, 14, 14, 512]               0\n",
      "        LayerNorm-65          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-66          [-1, 14, 14, 512]               0\n",
      "  StochasticDepth-67          [-1, 14, 14, 512]               0\n",
      "        LayerNorm-68          [-1, 14, 14, 512]           1,024\n",
      "           Linear-69         [-1, 14, 14, 2048]       1,050,624\n",
      "             GELU-70         [-1, 14, 14, 2048]               0\n",
      "          Dropout-71         [-1, 14, 14, 2048]               0\n",
      "           Linear-72          [-1, 14, 14, 512]       1,049,088\n",
      "          Dropout-73          [-1, 14, 14, 512]               0\n",
      "  StochasticDepth-74          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-75          [-1, 14, 14, 512]               0\n",
      "        LayerNorm-76          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-77          [-1, 14, 14, 512]               0\n",
      "  StochasticDepth-78          [-1, 14, 14, 512]               0\n",
      "        LayerNorm-79          [-1, 14, 14, 512]           1,024\n",
      "           Linear-80         [-1, 14, 14, 2048]       1,050,624\n",
      "             GELU-81         [-1, 14, 14, 2048]               0\n",
      "          Dropout-82         [-1, 14, 14, 2048]               0\n",
      "           Linear-83          [-1, 14, 14, 512]       1,049,088\n",
      "          Dropout-84          [-1, 14, 14, 512]               0\n",
      "  StochasticDepth-85          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-86          [-1, 14, 14, 512]               0\n",
      "        LayerNorm-87          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-88          [-1, 14, 14, 512]               0\n",
      "  StochasticDepth-89          [-1, 14, 14, 512]               0\n",
      "        LayerNorm-90          [-1, 14, 14, 512]           1,024\n",
      "           Linear-91         [-1, 14, 14, 2048]       1,050,624\n",
      "             GELU-92         [-1, 14, 14, 2048]               0\n",
      "          Dropout-93         [-1, 14, 14, 2048]               0\n",
      "           Linear-94          [-1, 14, 14, 512]       1,049,088\n",
      "          Dropout-95          [-1, 14, 14, 512]               0\n",
      "  StochasticDepth-96          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-97          [-1, 14, 14, 512]               0\n",
      "        LayerNorm-98          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-99          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-100          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-101          [-1, 14, 14, 512]           1,024\n",
      "          Linear-102         [-1, 14, 14, 2048]       1,050,624\n",
      "            GELU-103         [-1, 14, 14, 2048]               0\n",
      "         Dropout-104         [-1, 14, 14, 2048]               0\n",
      "          Linear-105          [-1, 14, 14, 512]       1,049,088\n",
      "         Dropout-106          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-107          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-108          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-109          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-110          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-111          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-112          [-1, 14, 14, 512]           1,024\n",
      "          Linear-113         [-1, 14, 14, 2048]       1,050,624\n",
      "            GELU-114         [-1, 14, 14, 2048]               0\n",
      "         Dropout-115         [-1, 14, 14, 2048]               0\n",
      "          Linear-116          [-1, 14, 14, 512]       1,049,088\n",
      "         Dropout-117          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-118          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-119          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-120          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-121          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-122          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-123          [-1, 14, 14, 512]           1,024\n",
      "          Linear-124         [-1, 14, 14, 2048]       1,050,624\n",
      "            GELU-125         [-1, 14, 14, 2048]               0\n",
      "         Dropout-126         [-1, 14, 14, 2048]               0\n",
      "          Linear-127          [-1, 14, 14, 512]       1,049,088\n",
      "         Dropout-128          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-129          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-130          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-131          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-132          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-133          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-134          [-1, 14, 14, 512]           1,024\n",
      "          Linear-135         [-1, 14, 14, 2048]       1,050,624\n",
      "            GELU-136         [-1, 14, 14, 2048]               0\n",
      "         Dropout-137         [-1, 14, 14, 2048]               0\n",
      "          Linear-138          [-1, 14, 14, 512]       1,049,088\n",
      "         Dropout-139          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-140          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-141          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-142          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-143          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-144          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-145          [-1, 14, 14, 512]           1,024\n",
      "          Linear-146         [-1, 14, 14, 2048]       1,050,624\n",
      "            GELU-147         [-1, 14, 14, 2048]               0\n",
      "         Dropout-148         [-1, 14, 14, 2048]               0\n",
      "          Linear-149          [-1, 14, 14, 512]       1,049,088\n",
      "         Dropout-150          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-151          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-152          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-153          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-154          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-155          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-156          [-1, 14, 14, 512]           1,024\n",
      "          Linear-157         [-1, 14, 14, 2048]       1,050,624\n",
      "            GELU-158         [-1, 14, 14, 2048]               0\n",
      "         Dropout-159         [-1, 14, 14, 2048]               0\n",
      "          Linear-160          [-1, 14, 14, 512]       1,049,088\n",
      "         Dropout-161          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-162          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-163          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-164          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-165          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-166          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-167          [-1, 14, 14, 512]           1,024\n",
      "          Linear-168         [-1, 14, 14, 2048]       1,050,624\n",
      "            GELU-169         [-1, 14, 14, 2048]               0\n",
      "         Dropout-170         [-1, 14, 14, 2048]               0\n",
      "          Linear-171          [-1, 14, 14, 512]       1,049,088\n",
      "         Dropout-172          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-173          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-174          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-175          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-176          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-177          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-178          [-1, 14, 14, 512]           1,024\n",
      "          Linear-179         [-1, 14, 14, 2048]       1,050,624\n",
      "            GELU-180         [-1, 14, 14, 2048]               0\n",
      "         Dropout-181         [-1, 14, 14, 2048]               0\n",
      "          Linear-182          [-1, 14, 14, 512]       1,049,088\n",
      "         Dropout-183          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-184          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-185          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-186          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-187          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-188          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-189          [-1, 14, 14, 512]           1,024\n",
      "          Linear-190         [-1, 14, 14, 2048]       1,050,624\n",
      "            GELU-191         [-1, 14, 14, 2048]               0\n",
      "         Dropout-192         [-1, 14, 14, 2048]               0\n",
      "          Linear-193          [-1, 14, 14, 512]       1,049,088\n",
      "         Dropout-194          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-195          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-196          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-197          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-198          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-199          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-200          [-1, 14, 14, 512]           1,024\n",
      "          Linear-201         [-1, 14, 14, 2048]       1,050,624\n",
      "            GELU-202         [-1, 14, 14, 2048]               0\n",
      "         Dropout-203         [-1, 14, 14, 2048]               0\n",
      "          Linear-204          [-1, 14, 14, 512]       1,049,088\n",
      "         Dropout-205          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-206          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-207          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-208          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-209          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-210          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-211          [-1, 14, 14, 512]           1,024\n",
      "          Linear-212         [-1, 14, 14, 2048]       1,050,624\n",
      "            GELU-213         [-1, 14, 14, 2048]               0\n",
      "         Dropout-214         [-1, 14, 14, 2048]               0\n",
      "          Linear-215          [-1, 14, 14, 512]       1,049,088\n",
      "         Dropout-216          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-217          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-218          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-219          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-220          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-221          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-222          [-1, 14, 14, 512]           1,024\n",
      "          Linear-223         [-1, 14, 14, 2048]       1,050,624\n",
      "            GELU-224         [-1, 14, 14, 2048]               0\n",
      "         Dropout-225         [-1, 14, 14, 2048]               0\n",
      "          Linear-226          [-1, 14, 14, 512]       1,049,088\n",
      "         Dropout-227          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-228          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-229          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-230          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-231          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-232          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-233          [-1, 14, 14, 512]           1,024\n",
      "          Linear-234         [-1, 14, 14, 2048]       1,050,624\n",
      "            GELU-235         [-1, 14, 14, 2048]               0\n",
      "         Dropout-236         [-1, 14, 14, 2048]               0\n",
      "          Linear-237          [-1, 14, 14, 512]       1,049,088\n",
      "         Dropout-238          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-239          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-240          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-241          [-1, 14, 14, 512]           1,024\n",
      "ShiftedWindowAttention-242          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-243          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-244          [-1, 14, 14, 512]           1,024\n",
      "          Linear-245         [-1, 14, 14, 2048]       1,050,624\n",
      "            GELU-246         [-1, 14, 14, 2048]               0\n",
      "         Dropout-247         [-1, 14, 14, 2048]               0\n",
      "          Linear-248          [-1, 14, 14, 512]       1,049,088\n",
      "         Dropout-249          [-1, 14, 14, 512]               0\n",
      " StochasticDepth-250          [-1, 14, 14, 512]               0\n",
      "SwinTransformerBlock-251          [-1, 14, 14, 512]               0\n",
      "       LayerNorm-252           [-1, 7, 7, 2048]           4,096\n",
      "          Linear-253           [-1, 7, 7, 1024]       2,097,152\n",
      "    PatchMerging-254           [-1, 7, 7, 1024]               0\n",
      "       LayerNorm-255           [-1, 7, 7, 1024]           2,048\n",
      "ShiftedWindowAttention-256           [-1, 7, 7, 1024]               0\n",
      " StochasticDepth-257           [-1, 7, 7, 1024]               0\n",
      "       LayerNorm-258           [-1, 7, 7, 1024]           2,048\n",
      "          Linear-259           [-1, 7, 7, 4096]       4,198,400\n",
      "            GELU-260           [-1, 7, 7, 4096]               0\n",
      "         Dropout-261           [-1, 7, 7, 4096]               0\n",
      "          Linear-262           [-1, 7, 7, 1024]       4,195,328\n",
      "         Dropout-263           [-1, 7, 7, 1024]               0\n",
      " StochasticDepth-264           [-1, 7, 7, 1024]               0\n",
      "SwinTransformerBlock-265           [-1, 7, 7, 1024]               0\n",
      "       LayerNorm-266           [-1, 7, 7, 1024]           2,048\n",
      "ShiftedWindowAttention-267           [-1, 7, 7, 1024]               0\n",
      " StochasticDepth-268           [-1, 7, 7, 1024]               0\n",
      "       LayerNorm-269           [-1, 7, 7, 1024]           2,048\n",
      "          Linear-270           [-1, 7, 7, 4096]       4,198,400\n",
      "            GELU-271           [-1, 7, 7, 4096]               0\n",
      "         Dropout-272           [-1, 7, 7, 4096]               0\n",
      "          Linear-273           [-1, 7, 7, 1024]       4,195,328\n",
      "         Dropout-274           [-1, 7, 7, 1024]               0\n",
      " StochasticDepth-275           [-1, 7, 7, 1024]               0\n",
      "SwinTransformerBlock-276           [-1, 7, 7, 1024]               0\n",
      " ConvTranspose2d-277          [-1, 512, 14, 14]       8,389,120\n",
      "     BatchNorm2d-278          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-279          [-1, 512, 14, 14]               0\n",
      " ConvTranspose2d-280          [-1, 256, 28, 28]       2,097,408\n",
      "     BatchNorm2d-281          [-1, 256, 28, 28]             512\n",
      "            ReLU-282          [-1, 256, 28, 28]               0\n",
      " ConvTranspose2d-283          [-1, 128, 56, 56]         524,416\n",
      "     BatchNorm2d-284          [-1, 128, 56, 56]             256\n",
      "            ReLU-285          [-1, 128, 56, 56]               0\n",
      " ConvTranspose2d-286         [-1, 64, 112, 112]         131,136\n",
      "     BatchNorm2d-287         [-1, 64, 112, 112]             128\n",
      "            ReLU-288         [-1, 64, 112, 112]               0\n",
      " ConvTranspose2d-289          [-1, 3, 224, 224]           3,075\n",
      "     BatchNorm2d-290          [-1, 3, 224, 224]               6\n",
      "            ReLU-291          [-1, 3, 224, 224]               0\n",
      "         Sigmoid-292          [-1, 3, 224, 224]               0\n",
      "         Decoder-293          [-1, 3, 224, 224]               0\n",
      "================================================================\n",
      "Total params: 69,858,249\n",
      "Trainable params: 69,858,249\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 534.79\n",
      "Params size (MB): 266.49\n",
      "Estimated Total Size (MB): 801.85\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (3,224,224), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'monai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmonai\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'monai'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'monai'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmonai\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'monai'"
     ]
    }
   ],
   "source": [
    "import monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.0+cu124\n",
      "MONAI version: 1.4.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import monai\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"MONAI version:\", monai.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from monai.networks.nets import SwinUNETR\n",
    "\n",
    "model = SwinUNETR(\n",
    "    img_size=(224, 224, 224),     \n",
    "    in_channels=3,             # 입력 채널 수 (예: 흑백 이미지면 1, 컬러면 3)\n",
    "    out_channels=3,            # 출력 채널 수 (예: 이진 분할일 경우 2)\n",
    "    feature_size=768,           # 모델의 기본 feature 크기\n",
    "    use_checkpoint=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
